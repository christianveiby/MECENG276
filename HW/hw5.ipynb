{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbdd670f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw5.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605adef2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1><center>SDSE Homework 5 <br><br> Text Classification with Naive Bayes </center></h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac00c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from hashutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc9dc36",
   "metadata": {},
   "source": [
    "In this homework we will apply the technique of Naive Bayes classification to the problem of categorizing text-based documents. The dataset is a selection of posts from scikit-learn's ['20 newsgroups' dataset](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset), which contains some 18000 newsgroup posts in 20 different categories, such as politics, autos, electronics, atheism, and hockey. For simplicity, we will focus on just two of the categories: computer graphics and motorcycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9f5d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hw5_text.pickle','rb') as file:\n",
    "    Xtrain, ytrain, Xval, yval, categories, vocabulary = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e818e7f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 1. Define constants\n",
    "\n",
    "Define the following variables in terms of quantities loaded from the pickle file. \n",
    "+ `N` ... number of documents in the training corpus\n",
    "+ `K` ... number of document categories\n",
    "+ `D` ... number of words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28a7f2b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = Xtrain.shape[0]\n",
    "K = len(categories)\n",
    "D = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f62cf5cb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1</pre></strong> passed! 💯</p>"
      ],
      "text/plain": [
       "q1 results: All test cases passed!"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae4bbb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 2. Find the number of training documents for each category\n",
    "\n",
    "Create a Python dictionary called `docs_per_category` that stores the number of documents in the training data under each category.\n",
    "The keys of `docs_per_category` are the two categories (`comp.graphics` and `rec.motorcycles`). The value for each key is an integer corresponding to the number of documents in that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a1bfa28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs_per_category = dict.fromkeys(categories,0)\n",
    "for category in categories:\n",
    "    docs_per_category[category] = np.sum(ytrain == category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ca04ee0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q2</pre></strong> passed! 🎉</p>"
      ],
      "text/plain": [
       "q2 results: All test cases passed!"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343cdeaf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 3. Bag-of-words representation\n",
    "\n",
    "Our Naive Bayes model will operate on a bag-of-words representation of each document. A bag-of-words representation is simply a set with all of the individual words of the document that are also in the vocabulary. The first task is to write the `to_bow` method, which converts a document into its bag-of-words representation.\n",
    "\n",
    "The arguments for this method are the document as a string and the vocabulary. It should return a set (`bow`) with the unique words that appear in both the document and the vocabulary. The comments in the method provide steps to follow.\n",
    "\n",
    "**Hints**\n",
    "+ The `set` constructor\n",
    "+ The `add` method on sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a01bc9a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_bow(doc,vocabulary):\n",
    "    bow = set()\n",
    "    \n",
    "    # Split `doc` at spaces using the the string's `split` method. Obtain a list of strings.\n",
    "    words_list = doc.split()\n",
    "\n",
    "    # Keep only unique words from the list, by casting it as a set.\n",
    "    word_set = set(words_list)\n",
    "\n",
    "    # From that set, store in bow only the ones that are present in the vocabulary.\n",
    "    bow = word_set.intersection(vocabulary)\n",
    "\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b697d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This code runs 'to_bow' on each of the documents in Xtrain. \n",
    "# It produces Xtrain_bow and bag_sizes\n",
    "\n",
    "# DO NOT MODIFY THIS CODE ...................\n",
    "Xtrain_bow = np.empty(len(Xtrain),dtype=set)\n",
    "bag_sizes = np.empty(len(Xtrain),dtype=int)\n",
    "for i, doc in enumerate(Xtrain):\n",
    "    bow = to_bow(doc,vocabulary)\n",
    "    Xtrain_bow[i] = bow\n",
    "    bag_sizes[i] = len(bow)\n",
    "# ..........................................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf729007",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q3</pre></strong> passed! 🙌</p>"
      ],
      "text/plain": [
       "q3 results: All test cases passed!"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef9596",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 4. Compute the document count for each word in each category\n",
    "To estimate the parameters of the Naive Bayes model, we will need to know, for each category and each word, the number of documents of the category that contain the word. This is the purpose of the `find_doc_counts_per_word_category` function.\n",
    "\n",
    "Implement the `find_doc_counts_per_word_category` following the steps provided in the code. This function accepts training data (`Xtrain_bow` and ` ytrain`), as well as the categories and the vocabulary. It produces `doc_counters`, which is a dictionary indexed by category. For each category, `doc_counters[category]` is a dictionary indexed by words in the vocabulary. For each `word` in the vocabulary, `doc_counters[category][word]` is the number of documents of that category that contain that word.\n",
    "\n",
    "For example, if we run the `find_doc_counts_per_word_category` and save the result as `doccount_per_cat_and_word`, then we can obtain the number of documents in the computer graphics category that contain the word \"windows\":\n",
    "\n",
    "```python \n",
    "doccount_per_cat_and_word = find_doc_counts_per_word_category(categories,vocabulary,ytrain,Xtrain_bow)\n",
    "doccount_per_cat_and_word['comp.graphics']['windows']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b25be969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_doc_counts_per_word_category(categories,vocabulary,ytrain,Xtrain_bow):\n",
    "\n",
    "    # Initialize doc_counters\n",
    "    doc_counters = dict.fromkeys(categories)\n",
    "    for category in categories:\n",
    "        doc_counters[category]  = dict.fromkeys(vocabulary,0)\n",
    "\n",
    "    # Loop through categories.\n",
    "    for category in categories:\n",
    "\n",
    "        # Filter Xtrain_bow and keep only the documents of this category\n",
    "        docs_in_category = [doc for doc, label in zip(Xtrain_bow, ytrain) if label == category]\n",
    "        # For each document in this category, increment the doc_counter entry for all vocabulary words found in the document.\n",
    "        for doc in docs_in_category:\n",
    "            for word in doc:\n",
    "                if word in vocabulary:\n",
    "                    doc_counters[category][word] += 1\n",
    "\n",
    "    return doc_counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1c8f495c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run `find_doc_counts_per_word_category` on every word in the training dataset\n",
    "\n",
    "# DO NOT MODIFY THIS CODE ...................\n",
    "doccount_per_cat_and_word = find_doc_counts_per_word_category(categories,vocabulary,ytrain,Xtrain_bow)\n",
    "# ..........................................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2adb7119",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q4</pre></strong> passed! 🍀</p>"
      ],
      "text/plain": [
       "q4 results: All test cases passed!"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e9e52",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 5. Find word frequencies per category\n",
    "\n",
    "Write the `compute_freq` method. This method takes `doccount_per_cat_and_word`, `ytrain` and the Laplace smoothing factor `alpha` as inputs and computes word and category frequencies.\n",
    "\n",
    "+ Category frequencies `catfreq[category]`: The category frequency for category $k$ is the proportion of the documents that are of class $k$.\n",
    "\n",
    "$$\\hat{p}_k = \\frac{N_k}{N} $$\n",
    "\n",
    "where $N_k$ is the number of documents in category $k$, and $N$ is the total number of documents. \n",
    "\n",
    "+ Word frequencies `wordfreq[category][word]`: The Laplace-smoothed word frequency for a word $d$ and category $k$.\n",
    "\n",
    "$$\\hat{p}_{d,k} = \\frac{N_{d,k}+\\alpha}{N_k + \\alpha D} $$\n",
    "\n",
    "where $N_{d,k}$ is the number of documents of category $k$ that contain word $d$, and $\\alpha$ is the Laplace smoothing factor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47c1810f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_freq(doccount_per_cat_and_word,ytrain,categories,vocabulary,alpha):\n",
    "\n",
    "    K = len(categories)  # number of categories\n",
    "    D = len(vocabulary)  # number of vocabulary words\n",
    "    N = len(ytrain)      # number of documents\n",
    "\n",
    "    # Compute the number of documents in each category. Store it in the dictionary `ndocs`.\n",
    "    ndocs = dict.fromkeys(categories)\n",
    "    for category in categories:\n",
    "        ndocs[category] = sum(1 for y in ytrain if y == category)\n",
    "\n",
    "    # Compute the category frequenies. For each category, catfreq[category] equals\n",
    "    # the number of documents of that category (ndocs) divided by the total number of documents.\n",
    "    catfreq = dict()\n",
    "    for category, n in ndocs.items():\n",
    "        catfreq[category] = n/N\n",
    "\n",
    "    # Initialize wordfreq\n",
    "    wordfreq = dict.fromkeys(categories)\n",
    "    for category in categories:\n",
    "        wordfreq[category] = dict.fromkeys(vocabulary)\n",
    "\n",
    "    # Compute wordfreq\n",
    "    # For each category ...\n",
    "    for category in categories:\n",
    "\n",
    "        # the denominator is the number of documents in that category + alpha*K\n",
    "        den = ndocs[category] + alpha * D\n",
    "\n",
    "        # iterate through items in `doccount_per_cat_and_word` to compute\n",
    "        # the word frequency for every category and word.\n",
    "        for word, doccount in doccount_per_cat_and_word[category].items():\n",
    "            wordfreq[category][word] = (doccount + alpha) / den\n",
    "\n",
    "    return wordfreq, catfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "35346c6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run `compute_word_log_freq` with alpha=0.01.\n",
    "\n",
    "# DO NOT MODIFY THIS CODE ...................\n",
    "wordfreq, catfreq = compute_freq(doccount_per_cat_and_word,ytrain,categories,vocabulary,0.01)\n",
    "# ..........................................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e5217b73",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q5</pre></strong> passed! 🌈</p>"
      ],
      "text/plain": [
       "q5 results: All test cases passed!"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab652574",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 6. Write the Naive Bayes prediction function.\n",
    "\n",
    "Use your Naive Bayes model to predict the category of a given test document `doc`. \n",
    "\n",
    "Recall that Naive Bayes selects the category as follows:\n",
    "\n",
    "$$\\hat{y} = \\underset{k}{\\text{argmax}} \\hspace{2mm} \\log \\hat{p}_k +   \\sum_{d:x^d=1}  \\log \\hat{p}_{d,k} +\\sum_{d:x^d=0}  \\log (1-\\hat{p}_{d,k})$$\n",
    "\n",
    "The arguments of the `predict` method are:\n",
    "+ `doc`: a single document as a string.\n",
    "+ `wordfreq`, `catfreq`: the ratios computed in the previous step (with $\\alpha=0.01$)\n",
    "+ `vocabulary`: the vocabulary.\n",
    "\n",
    "The steps are:\n",
    "1. Find the BOW representation of `doc`.\n",
    "\n",
    "2. Loop through categories, for each one compute its score using the above formula.\n",
    "\n",
    "3. Return the category with the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ed39c645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def predict(doc, wordfreq, catfreq, vocabulary):\n",
    "    # 1. Find the BOW representation of doc.\n",
    "    doc_bow = to_bow(doc, vocabulary)\n",
    "    \n",
    "    # 2. Loop through categories, for each one compute its score, and save it in score_cat.\n",
    "    score_cat = dict.fromkeys(catfreq.keys(), 0)\n",
    "    epsilon = 1e-10  # Small value to prevent log(0)\n",
    "    \n",
    "    for category in catfreq.keys():\n",
    "        # Start with the log of the category frequency\n",
    "        score = math.log(catfreq[category] + epsilon)\n",
    "        \n",
    "        for word in vocabulary:\n",
    "            p_dk = wordfreq[category][word]\n",
    "            # Ensure probabilities are within (epsilon, 1 - epsilon)\n",
    "            p_dk = min(max(p_dk, epsilon), 1 - epsilon)\n",
    "            \n",
    "            if word in doc_bow:\n",
    "                # Word is present in document\n",
    "                score += math.log(p_dk)\n",
    "            else:\n",
    "                # Word is not present in document\n",
    "                score += math.log(1 - p_dk)\n",
    "        \n",
    "        score_cat[category] = score\n",
    "    \n",
    "    # 3. Find the category with the highest score.\n",
    "    maxcat = max(score_cat, key=score_cat.get)\n",
    "    \n",
    "    return maxcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5a1959a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rec.motorcycles'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run `predict` on all of the documents in the validation set.\n",
    "\n",
    "# DO NOT MODIFY THIS CODE ...................\n",
    "allpred = list()\n",
    "for x in Xval:\n",
    "    allpred.append( predict(x, wordfreq, catfreq, vocabulary) )\n",
    "# ...........................................\n",
    "\n",
    "allpred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dfe63316",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q6</pre></strong> passed! 🍀</p>"
      ],
      "text/plain": [
       "q6 results: All test cases passed!"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72c12cc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 7. Compute the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91333aaa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The function `compute_accuracy` takes a dataset (`X`,`y`), computes predictions for the documents in `X` using the `predict` function, and computes the accuracy of these predictions with respect to `y`. The accuracy of the model is defined as the number of correct predictions, divided by the total number of predictions. Implement this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c736baa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(X, y, wordfreq, catfreq, vocabulary):\n",
    "\n",
    "    # count the number of correct predictions\n",
    "    correct = 0\n",
    "    total_predictions = len(X)\n",
    "    for i in range(len(X)):\n",
    "        # Predict the category for the i-th document\n",
    "        predicted_category = predict(X[i], wordfreq, catfreq, vocabulary)\n",
    "        \n",
    "        # Compare with the true label\n",
    "        if predicted_category == y[i]:\n",
    "            correct += 1\n",
    "\n",
    "    # Accuracy is the ratio of correct predictions to total predictions\n",
    "    accuracy = correct / total_predictions\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "386e2064",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(Xtrain, ytrain, wordfreq, catfreq, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0391bb21",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q7</pre></strong> passed! 🙌</p>"
      ],
      "text/plain": [
       "q7 results: All test cases passed!"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f522bc26",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "Make sure you submit the .zip file to Gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0d72cc3e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <p>Your submission has been exported. Click <a href=\"hw5_2024_11_25T11_43_42_476088.zip\" download=\"hw5_2024_11_25T11_43_42_476088.zip\" target=\"_blank\">here</a>\n",
       "            to download the zip file.</p>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a290430b",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "otter": {
   "OK_FORMAT": true,
   "assignment_name": "hw5",
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> get_hash(N, 2) == '62dca49f0781bf26b4305bddb0414bea'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> get_hash(K, 2) == 'd1bd83a33f1a841ab7fda32449746cc4'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> get_hash(D, 3) == '6cc3577970e97938a2191e0443243dab'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> get_hash(docs_per_category['comp.graphics'], 3) == '140c93142d2695247f269ee4af3001b0'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> get_hash(docs_per_category['rec.motorcycles'], 3) == '73aa23ef8363441b157cde7d6cd0f634'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> get_hash(''.join([''.join(np.sort(list(bow))) for bow in Xtrain_bow])) == '62075cf30caf7a8196c1d8e97ef68d5d'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> get_hash(bag_sizes, 3) == '597702cfceb8c3ef0616f259be56a36d'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 3
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> doccount_per_cat_and_word = find_doc_counts_per_word_category(categories, vocabulary, ytrain, Xtrain_bow)\n>>> len(doccount_per_cat_and_word['rec.motorcycles']) == 548 and doccount_per_cat_and_word['rec.motorcycles']['first'] == 5\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> doccount_per_cat_and_word = find_doc_counts_per_word_category(categories, vocabulary, ytrain, Xtrain_bow)\n>>> testwords = ['according', 'between', 'could', 'explain', 'harley', 'miles', 'source', 'wondering']\n>>> get_hash([doccount_per_cat_and_word['comp.graphics'][word] for word in testwords], 2) == '15e5b3e1d64da1dbd9557db56fe89d31'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> doccount_per_cat_and_word = find_doc_counts_per_word_category(categories, vocabulary, ytrain, Xtrain_bow)\n>>> testwords = ['according', 'between', 'could', 'explain', 'harley', 'miles', 'source', 'wondering']\n>>> get_hash([doccount_per_cat_and_word['rec.motorcycles'][word] for word in testwords], 2) == 'e12cdf40fcd60f51698f335252c3ddc4'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 3
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> (wordfreq, catfreq) = compute_freq(doccount_per_cat_and_word, ytrain, categories, vocabulary, 0.01)\n>>> bool(len(wordfreq) == 2 and len(catfreq) == 2 and np.isclose(wordfreq['comp.graphics']['does'], 0.11522, atol=0.001))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> bool(catfreq['comp.graphics'] > 0.3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> get_hash(catfreq['comp.graphics'], 4) == 'afbc48a7ca8d716f9efa7cc993316668'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> (wordfreq, catfreq) = compute_freq(doccount_per_cat_and_word, ytrain, categories, vocabulary, 0.01)\n>>> testwords = ['according', 'between', 'could', 'explain', 'harley', 'miles', 'source', 'wondering']\n>>> get_hash([float(wordfreq['rec.motorcycles'][word]) for word in testwords], 4) == '4c02fc41f94271e20664441d66a3a4b4'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> (wordfreq, catfreq) = compute_freq(doccount_per_cat_and_word, ytrain, categories, vocabulary, 0.01)\n>>> testwords = ['according', 'between', 'could', 'explain', 'harley', 'miles', 'source', 'wondering']\n>>> get_hash([float(wordfreq['comp.graphics'][word]) for word in testwords], 4) == '0282682c12c6023bdb6840f03a69d989'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> allpred = [predict(x, wordfreq, catfreq, vocabulary) for x in Xval]\n>>> allpred = ''.join([a[0] for a in allpred])\n>>> get_hash(allpred) == '68d7a8dffe1ac1b0fb68d1afc9c45da8'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 4
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> acc0 = compute_accuracy(Xtrain, ytrain, wordfreq, catfreq, vocabulary)\n>>> bool(acc0 > 0.9 and acc0 < 1.05)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> acc1 = compute_accuracy(Xval, yval, wordfreq, catfreq, vocabulary)\n>>> get_hash(acc1, 3) == 'a894124cc6d5c5c71afe060d5dde0762'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
